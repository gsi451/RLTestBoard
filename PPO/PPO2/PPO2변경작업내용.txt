import gym
from gym import spaces
import numpy as np
import pandas as pd
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from sklearn.preprocessing import StandardScaler
from stable_baselines3.common.callbacks import BaseCallback
import torch
from torch.utils.tensorboard import SummaryWriter

############################
# ppo 강화학습
# PPO1
############################

############################
# 데이터 불러오기 및 분할
############################

df = pd.read_csv('g_1min_m_add_2024112500_2025022313.csv')
df['timestamp'] = pd.to_datetime(df['timestamp'])

# 원하는 날짜 구간으로 분할
train_df = df[(df['timestamp'] >= '2024-11-26') & (df['timestamp'] <= '2025-01-15')].copy()
val_df   = df[(df['timestamp'] >= '2025-01-16') & (df['timestamp'] <= '2025-01-31')].copy()
test_df  = df[(df['timestamp'] >= '2025-02-01') & (df['timestamp'] <= '2025-02-22')].copy()

train_df.reset_index(drop=True, inplace=True)
val_df.reset_index(drop=True, inplace=True)
test_df.reset_index(drop=True, inplace=True)

FEATURE_COLUMNS = ['open','high','low','close','volume']

###############################################################
# (옵션) 거래량 로그 변환 및 전체 피처 표준화 (Train->Val/Test)
###############################################################

# 거래량 로그 변환
for df_ in [train_df, val_df, test_df]:
    df_['volume'] = np.log1p(df_['volume'])

# 표준화
scaler = StandardScaler()
scaler.fit(train_df[FEATURE_COLUMNS])
for df_ in [train_df, val_df, test_df]:
    df_[FEATURE_COLUMNS] = scaler.transform(df_[FEATURE_COLUMNS])

################################
# 강화학습 환경(TradingEnv)
################################

class TradingEnv(gym.Env):
    def __init__(self, df: pd.DataFrame, lookback=1440, initial_balance=10000.0, 
                 buy_fee_rate=0.00002, sell_fee_rate=0.00003, slippage=0.0005,
                 max_holding_duration=180):  # 3시간 = 180분
        super(TradingEnv, self).__init__()
        
        self.df = df
        self.lookback = lookback  # 하루치 데이터를 관측값으로 사용
        self.max_step = len(df) - 1
        self.current_step = self.lookback
        
        self.initial_balance = initial_balance
        self.balance = self.initial_balance
        self.holding = 0.0  # 비율 기반
        self.net_worth = self.balance
        
        self.buy_fee_rate = buy_fee_rate
        self.sell_fee_rate = sell_fee_rate
        self.slippage = slippage  # 매수/매도 가격 변동 반영
        
        # 보유 기간 제한 변수
        self.max_holding_duration = max_holding_duration
        self.holding_duration = 0
        
        # 에피소드 누적 수익 추적 (실제 금액 단위)
        self.episode_profit = 0.0
        
        # (lookback, len(FEATURE_COLUMNS))
        self.observation_space = spaces.Box(
            low=-np.inf, 
            high=np.inf, 
            shape=(lookback, len(FEATURE_COLUMNS)), 
            dtype=np.float32
        )
        
        self.action_space = spaces.Box(low=0, high=1, shape=(1,), dtype=np.float32)

    def _get_current_price(self):
        return self.df.loc[self.current_step, 'close']
    
    def _get_obs(self):
        obs = self.df.loc[self.current_step - self.lookback:self.current_step-1, FEATURE_COLUMNS].values
        return obs.astype(np.float32)
    
    def _update_net_worth(self, current_price):
        self.net_worth = self.balance + (self.holding * current_price)

    def step(self, action):
        prev_net_worth = self.net_worth
        current_price = self._get_current_price()
        
        trade_amount = action[0]  # 0~1 비율
        
        # 거래 타입 초기화 (매수, 매도, 홀드)
        trade_type = "hold"
        if trade_amount > 0.5:  # 매수
            trade_type = "buy"
            amount = (trade_amount - 0.5) * 2  # 0.5~1.0 -> 0~1 변환
            cost = amount * current_price * (1 + self.buy_fee_rate + self.slippage)
            if self.balance >= cost:
                self.balance -= cost
                self.holding += amount
        elif trade_amount < 0.5:  # 매도
            trade_type = "sell"
            amount = (0.5 - trade_amount) * 2  # 0~0.5 -> 0~1 변환
            proceeds = amount * current_price * (1 - self.sell_fee_rate - self.slippage)
            if self.holding >= amount:
                self.balance += proceeds
                self.holding -= amount

        # 보유 상태에 따른 보유 시간 업데이트
        if self.holding > 0:
            self.holding_duration += 1
        else:
            self.holding_duration = 0

        # 보유 기간 초과 시 강제 매도 처리 후 에피소드 종료
        if self.holding_duration >= self.max_holding_duration:
            # 보유한 모든 포지션을 현재 가격에 강제 매도
            if self.holding > 0:
                amount = self.holding
                proceeds = amount * current_price * (1 - self.sell_fee_rate - self.slippage)
                self.balance += proceeds
                self.holding = 0.0
            done = True
        else:
            self.current_step += 1
            done = self.current_step >= self.max_step
        
        self._update_net_worth(current_price)
        
        # 실제 금액 차이를 reward로 사용 (예: -100, +1000 등)
        reward = (self.net_worth - prev_net_worth)
        
        # 에피소드 누적 수익 업데이트
        self.episode_profit += reward
        
        obs = self._get_obs()
        
        info = {
            'step': self.current_step, 
            'balance': self.balance, 
            'holding': self.holding, 
            'net_worth': self.net_worth, 
            'holding_duration': self.holding_duration,
            'trade_type': trade_type,       # "buy", "sell", 또는 "hold"
            'reward': reward                # 해당 step의 보상값 (실제 금액 차이)
        }
        # 에피소드 종료 시 누적 수익을 info에 추가
        if done:
            info['episode_profit'] = self.episode_profit
        return obs, reward, done, info

    def reset(self):
        self.current_step = self.lookback
        self.balance = self.initial_balance
        self.holding = 0.0
        self.net_worth = self.balance
        self.holding_duration = 0
        self.episode_profit = 0.0
        return self._get_obs()

###############################################
# 학습, 검증, 테스트 환경 준비
###############################################

train_env = DummyVecEnv([lambda: TradingEnv(train_df)])
val_env   = DummyVecEnv([lambda: TradingEnv(val_df)])
test_env  = DummyVecEnv([lambda: TradingEnv(test_df)])

###############################################
# 텐서보드에 커스텀 metric 기록하는 Callback
###############################################

class TensorboardCallback(BaseCallback):
    def __init__(self, verbose=0):
        super(TensorboardCallback, self).__init__(verbose)
    
    def _on_step(self) -> bool:
        infos = self.locals.get("infos", [])
        rewards = self.locals.get("rewards", [])
        dones = self.locals.get("dones", [])
        
        if infos:
            info = infos[0]
            # 현재 step의 실제 보상 (금액)
            step_reward = rewards[0] if rewards else 0.0
            self.logger.record("custom/step_reward", step_reward)
            # 현재 net_worth 기록
            net_worth = info.get("net_worth", 0)
            self.logger.record("custom/net_worth", net_worth)
            # 거래 타입 indicator 기록
            trade_type = info.get("trade_type", "hold")
            buy_flag = 1 if trade_type == "buy" else 0
            sell_flag = 1 if trade_type == "sell" else 0
            hold_flag = 1 if trade_type == "hold" else 0
            self.logger.record("custom/buy_flag", buy_flag)
            self.logger.record("custom/sell_flag", sell_flag)
            self.logger.record("custom/hold_flag", hold_flag)
            # 에피소드 종료 시 누적 수익 기록
            if dones and dones[0] and "episode_profit" in info:
                ep_profit = info["episode_profit"]
                self.logger.record("custom/episode_profit", ep_profit)
            
            # 1000 스텝마다 모델 파라미터 히스토그램 기록
            if self.n_calls % 1000 == 0:
                # logger.output_formats에서 TensorBoard writer를 찾습니다.
                for name, param in self.model.policy.named_parameters():
                    for output_format in self.logger.output_formats:
                        if hasattr(output_format, "writer"):
                            output_format.writer.add_histogram(
                                f"parameters/{name}",
                                param.detach().cpu().numpy(),
                                self.num_timesteps
                            )
        return True

###############################################
# PPO 모델 설정 및 학습
###############################################

model = PPO(
    "MlpPolicy",
    train_env,
    verbose=1,
    tensorboard_log="../tensorboard_log/",
    n_steps=3200,
    gamma=0.99,
    gae_lambda=0.95,
    ent_coef=0.02,
    learning_rate=5e-5,
    clip_range=0.15
)

# 학습 시 콜백을 통해 커스텀 metric 기록
model.learn(total_timesteps=20000000, callback=TensorboardCallback())
print("----- 학습 완료 -----")

# 학습된 모델 저장
model.save("ppo_trading_model_ppo1")
print("모델 저장 완료!")

###############################################
# 검증 (Validation) & 테스트 (Test) 시뮬레이션
###############################################

for env, name in zip([val_env, test_env], ["Validation", "Test"]):
    print(f"\n===== {name} 시뮬레이션 =====")
    obs = env.reset()
    done = False
    while not done:
        action, _ = model.predict(obs)
        obs, reward, done, info = env.step(action)
    print(f"{name} 종료. 마지막 info:", info)

강화학습에 사용된 소스야
여기 소스에서 아래 부분을 수정할건데
수정하게 되면 변경되는 부분들이 어디인지 알려줘


# 거래량 로그 변환
for df_ in [train_df, val_df, test_df]:
    df_['volume'] = np.log1p(df_['volume'])

# 표준화
scaler = StandardScaler()
scaler.fit(train_df[FEATURE_COLUMNS])
for df_ in [train_df, val_df, test_df]:
    df_[FEATURE_COLUMNS] = scaler.transform(df_[FEATURE_COLUMNS])

여기 표준화 하는 부분을 


# sample.csv 파일 읽기
df = pd.read_csv("sample.csv", parse_dates=["timestamp"])
df.set_index("timestamp", inplace=True)

# 로그수익률 변환 (첫 번째 값은 NaN이 됨)
for col in ["open", "high", "low", "close"]:
    df[f"log_return_{col}"] = np.log(df[col] / df[col].shift(1))

# 거래량 로그 변환
df["log_volume"] = np.log1p(df["volume"])

# 거래량 표준화 (로그 변환된 값 기준)
scaler = StandardScaler()
df["log_scaled_volume"] = scaler.fit_transform(df[["log_volume"]])

# 로그수익률을 적용한 새로운 가격 데이터 생성 (기준점 대비 상대 변화율을 누적)
df["adj_close"] = df["close"].iloc[0] * np.exp(df["log_return_close"].cumsum())
df["adj_open"] = df["open"].iloc[0] * np.exp(df["log_return_open"].cumsum())
df["adj_high"] = df["high"].iloc[0] * np.exp(df["log_return_high"].cumsum())
df["adj_low"] = df["low"].iloc[0] * np.exp(df["log_return_low"].cumsum())

# 새로운 데이터프레임 생성 (변환된 가격 데이터 사용)
ohlc_df = df[["adj_open", "adj_high", "adj_low", "adj_close", "volume"]].dropna()
ohlc_df.columns = ["open", "high", "low", "close", "volume"]  # 캔들차트 호환을 위해 이름 변경

# 캔들차트 스타일 지정
style = mpf.make_mpf_style(base_mpf_style="charles", rc={"figure.figsize": (10, 8)})

여기 소스에 있는 표준화 기법으로 변경하고 싶어
21초 동안 이성 처리 완료
아래는 기존 표준화 및 변환 코드와 새 코드의 차이로 인해 변경되는 부분들입니다.

데이터 읽기 방식

기존:
pd.read_csv('g_1min_m_add_2024112500_2025022313.csv')로 데이터를 읽고, timestamp 컬럼을 datetime으로 변환한 후 train/validation/test로 분할함.
변경 후:
"sample.csv" 파일을 읽으며 parse_dates=["timestamp"] 옵션을 사용하고, df.set_index("timestamp", inplace=True)로 타임스탬프를 인덱스로 설정합니다.
영향: 데이터 로딩 및 인덱스 설정 방식, 그리고 이후 분할(현재는 분할 코드가 사라짐)에 변화가 생깁니다.
가격 데이터 변환 (로그수익률 계산 및 누적 적용)

기존:
가격 데이터(open, high, low, close)는 별도의 변환 없이 그대로 사용됨.
변경 후:
각 가격 컬럼에 대해 np.log(df[col] / df[col].shift(1))를 이용한 로그수익률(log_return_컬럼명)을 계산합니다.
이후, 로그수익률의 누적합을 이용해 새로운 가격 데이터(adj_open, adj_high, adj_low, adj_close)를 생성합니다.
영향: 모델이 사용하는 가격 정보가 원본 가격에서 누적 로그수익률을 반영한 조정가격으로 바뀝니다.
거래량 처리 방식 변경

기존:
모든 데이터프레임(train, val, test)에 대해 df_['volume'] = np.log1p(df_['volume'])로 로그 변환 후, FEATURE_COLUMNS 전체(가격과 거래량)를 표준화함.
변경 후:
df["log_volume"] = np.log1p(df["volume"])로 거래량 로그 변환을 수행합니다.
이후, StandardScaler를 적용해 df["log_scaled_volume"] 컬럼에 표준화된 값을 저장합니다.
영향:
표준화 대상이 전체 FEATURE_COLUMNS에서 오직 거래량의 로그값으로 축소됩니다.
결과적으로, 기존 가격 데이터는 변환되지 않고, 거래량은 로그변환 후 별도의 표준화된 값(log_scaled_volume)을 갖게 됩니다.
최종 데이터프레임 구성

기존:
train_df, val_df, test_df로 분할되어 각각의 FEATURE_COLUMNS가 그대로 사용됨.
변경 후:
로그수익률 및 조정가격을 반영한 새로운 데이터프레임 ohlc_df를 생성합니다.
ohlc_df는 ["adj_open", "adj_high", "adj_low", "adj_close", "volume"] 컬럼을 사용한 후, 캔들차트와 호환되도록 컬럼명을 ["open", "high", "low", "close", "volume"]로 재설정합니다.
영향: 모델이 학습할 때 사용되는 데이터프레임의 구성 및 컬럼명이 변경됩니다.
캔들차트 스타일 지정 추가

변경 후 코드에서는 mpf.make_mpf_style을 사용해 캔들차트의 스타일을 지정하는 코드가 추가됩니다.
영향: 시각화를 진행할 경우, 캔들차트의 스타일이 새롭게 지정됩니다.
요약하면, CSV 파일의 읽기 및 인덱스 설정, 데이터 분할(또는 통합), 가격 데이터에 대한 로그수익률 및 누적 계산, 거래량에 대한 로그변환 및 표준화 대상 변경, 그리고 최종적으로 모델 입력 데이터프레임 구성과 캔들차트 스타일 지정 부분에서 수정이 이루어지게 됩니다.






나의 말:
sample.csv 부분으 참고만해


df = pd.read_csv('g_1min_m_add_2024112500_2025022313.csv')
df['timestamp'] = pd.to_datetime(df['timestamp'])

# 원하는 날짜 구간으로 분할
train_df = df[(df['timestamp'] >= '2024-11-26') & (df['timestamp'] <= '2025-01-15')].copy()
val_df   = df[(df['timestamp'] >= '2025-01-16') & (df['timestamp'] <= '2025-01-31')].copy()
test_df  = df[(df['timestamp'] >= '2025-02-01') & (df['timestamp'] <= '2025-02-22')].copy()

이 원래 코드로 사용할거야

다시 분석해줘
25초 동안 이성 처리 완료
아래는 원래 데이터 불러오기 및 분할 코드는 그대로 유지하면서,
기존의 거래량 로그 변환 및 전체 피처 표준화 방식을 아래와 같이 새 방식(샘플 코드 참고)으로 대체할 경우 변경되는 부분입니다.

1. 기존 코드
python
복사
# 거래량 로그 변환
for df_ in [train_df, val_df, test_df]:
    df_['volume'] = np.log1p(df_['volume'])

# 표준화
scaler = StandardScaler()
scaler.fit(train_df[FEATURE_COLUMNS])
for df_ in [train_df, val_df, test_df]:
    df_[FEATURE_COLUMNS] = scaler.transform(df_[FEATURE_COLUMNS])
2. 변경 후(샘플 코드 참조) 적용 방식
가격 데이터 처리

각 분할 데이터셋(train_df, val_df, test_df)에 대해 "open", "high", "low", "close" 컬럼에 대해 로그수익률를 계산합니다.
python
복사
for col in ["open", "high", "low", "close"]:
    df_[f"log_return_{col}"] = np.log(df_[col] / df_[col].shift(1))
계산한 로그수익률의 누적합을 사용하여 새로운 조정 가격 데이터를 생성합니다.
python
복사
df_["adj_open"]  = df_["open"].iloc[0] * np.exp(df_["log_return_open"].cumsum())
df_["adj_high"]  = df_["high"].iloc[0] * np.exp(df_["log_return_high"].cumsum())
df_["adj_low"]   = df_["low"].iloc[0]  * np.exp(df_["log_return_low"].cumsum())
df_["adj_close"] = df_["close"].iloc[0] * np.exp(df_["log_return_close"].cumsum())
변경 효과:
기존에는 원본 가격 데이터를 그대로 사용했지만, 이제는 기준점 대비 상대 변화율(로그수익률의 누적)을 반영한 조정 가격 데이터를 사용하게 됩니다.
거래량 처리

거래량에 대해 로그 변환을 수행한 후
python
복사
df_["log_volume"] = np.log1p(df_["volume"])
해당 로그 변환된 값을 StandardScaler로 표준화하여 새로운 컬럼(예, "log_scaled_volume")에 저장합니다.
python
복사
scaler = StandardScaler()
df_["log_scaled_volume"] = scaler.fit_transform(df_[["log_volume"]])
변경 효과:
기존에는 거래량만 로그 변환하여 FEATURE_COLUMNS에 포함시켰지만, 이제는 거래량을 별도로 표준화한 결과를 저장합니다.
최종 데이터프레임 구성 및 컬럼 명 재설정

각 데이터셋에서 NaN 제거 후, 모델에 사용하기 위해 조정 가격 컬럼들을 원래의 "open", "high", "low", "close" 이름으로 재설정할 수 있습니다.
python
복사
df_ = df_.dropna(subset=["adj_open", "adj_high", "adj_low", "adj_close"])
df_.rename(columns={
    "adj_open": "open", 
    "adj_high": "high", 
    "adj_low": "low", 
    "adj_close": "close"
}, inplace=True)
변경 효과:
모델 입력으로 사용되는 데이터프레임이 기존의 원본 가격 대신, 로그수익률을 반영한 조정 가격과 표준화된 거래량(또는 필요에 따라 원래 거래량)을 포함하도록 변경됩니다.
3. 요약
변경 전:

모든 피처(가격, 거래량)를 단순히 StandardScaler로 표준화
거래량은 np.log1p만 적용
변경 후:

가격은 각 컬럼별 로그수익률을 계산한 후 누적합을 적용해 조정 가격으로 변환
거래량은 로그 변환 후 개별적으로 StandardScaler를 적용하여 표준화한 값을 생성
최종적으로 모델 학습에 사용될 데이터는 조정 가격(이름을 재설정)과 거래량(또는 log_scaled_volume)을 포함하게 됩니다.
이와 같이 표준화 및 변환 기법이 변경되며, 데이터 전처리 흐름과 최종 피처 구성이 달라집니다.






나의 말:
나머지 강화학습 환경 부분이나 하이퍼파라미터 부분은 변경하지 않아도 되는거야?
6초 동안 이성 처리 완료
네, 기본적으로 강화학습 환경 및 하이퍼파라미터 부분은 그대로 두어도 됩니다. 다만, 전처리 방식이 변경되면서 입력 데이터의 컬럼 구성이나 순서가 달라질 수 있으므로, observation space나 FEATURE_COLUMNS 등 환경에서 데이터를 참조하는 부분이 새 데이터 구조와 맞는지 확인해야 합니다. 예를 들어, 조정 가격이나 표준화된 거래량을 사용하게 된다면 해당 컬럼 이름과 순서를 환경에서도 일관되게 적용해줘야 합니다.






나의 말:
표준화된 정보를 시가, 고가, 저가, 종가, 거래량으로 치환해서 사용이 가능한거지?
몇 초 동안 이성 처리 완료
네, 가능합니다.
단, 전처리 후에 생성된 데이터의 컬럼 순서와 이름이 강화학습 환경에서 사용되는 FEATURE_COLUMNS(시가, 고가, 저가, 종가, 거래량)와 일치하도록 매핑되어야 합니다.
예를 들어, 조정 가격 데이터를 생성한 후 컬럼명을 "open", "high", "low", "close"로 재설정하고, 거래량도 동일한 방식으로 처리하면 기존 환경 및 하이퍼파라미터와 호환하여 사용이 가능합니다.






나의 말:
여기까지 정리된 내용을 토대로 코드 작성해줘
27초 동안 이성 처리 완료
아래는 지금까지의 내용을 반영하여, 원래 데이터 분할 코드는 그대로 사용하면서 표준화 및 변환 기법을 변경한 전체 코드 예시입니다.

python
복사
import gym
from gym import spaces
import numpy as np
import pandas as pd
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from sklearn.preprocessing import StandardScaler
from stable_baselines3.common.callbacks import BaseCallback
import torch
from torch.utils.tensorboard import SummaryWriter
import mplfinance as mpf  # 캔들차트 스타일 지정용 (필요시 사용)

###############################################
# 데이터 불러오기 및 분할 (원래 코드 그대로 사용)
###############################################

df = pd.read_csv('g_1min_m_add_2024112500_2025022313.csv')
df['timestamp'] = pd.to_datetime(df['timestamp'])

# 원하는 날짜 구간으로 분할
train_df = df[(df['timestamp'] >= '2024-11-26') & (df['timestamp'] <= '2025-01-15')].copy()
val_df   = df[(df['timestamp'] >= '2025-01-16') & (df['timestamp'] <= '2025-01-31')].copy()
test_df  = df[(df['timestamp'] >= '2025-02-01') & (df['timestamp'] <= '2025-02-22')].copy()

train_df.reset_index(drop=True, inplace=True)
val_df.reset_index(drop=True, inplace=True)
test_df.reset_index(drop=True, inplace=True)

###############################################
# 데이터 전처리: 로그수익률, 조정 가격 및 거래량 표준화
###############################################

def preprocess_df(df):
    # 가격 컬럼별 로그수익률 계산 (첫 행은 NaN)
    for col in ["open", "high", "low", "close"]:
        df[f"log_return_{col}"] = np.log(df[col] / df[col].shift(1))
    
    # 누적 로그수익률을 통해 조정 가격 계산
    df["adj_open"]  = df["open"].iloc[0]  * np.exp(df["log_return_open"].cumsum())
    df["adj_high"]  = df["high"].iloc[0]  * np.exp(df["log_return_high"].cumsum())
    df["adj_low"]   = df["low"].iloc[0]   * np.exp(df["log_return_low"].cumsum())
    df["adj_close"] = df["close"].iloc[0] * np.exp(df["log_return_close"].cumsum())
    
    # 거래량 로그 변환
    df["log_volume"] = np.log1p(df["volume"])
    
    return df

# 각 분할 데이터프레임에 동일하게 전처리 적용
train_df = preprocess_df(train_df)
val_df = preprocess_df(val_df)
test_df = preprocess_df(test_df)

# 로그수익률 계산으로 발생하는 NaN 제거 및 인덱스 재설정
train_df = train_df.dropna().reset_index(drop=True)
val_df   = val_df.dropna().reset_index(drop=True)
test_df  = test_df.dropna().reset_index(drop=True)

# 거래량 표준화 (훈련 데이터 기준으로 fit 후, 검증/테스트 데이터에 transform 적용)
volume_scaler = StandardScaler()
train_df["log_scaled_volume"] = volume_scaler.fit_transform(train_df[["log_volume"]])
val_df["log_scaled_volume"]   = volume_scaler.transform(val_df[["log_volume"]])
test_df["log_scaled_volume"]  = volume_scaler.transform(test_df[["log_volume"]])

# 조정된 가격 데이터를 원래의 컬럼 이름으로 재설정하고, 거래량은 표준화된 값으로 대체
def finalize_df(df):
    df_final = df[["adj_open", "adj_high", "adj_low", "adj_close", "log_scaled_volume"]].copy()
    df_final.columns = ["open", "high", "low", "close", "volume"]
    return df_final

train_df = finalize_df(train_df)
val_df   = finalize_df(val_df)
test_df  = finalize_df(test_df)

# 강화학습 환경에서 사용될 피처 컬럼
FEATURE_COLUMNS = ['open', 'high', 'low', 'close', 'volume']

###############################################
# 강화학습 환경(TradingEnv)
###############################################

class TradingEnv(gym.Env):
    def __init__(self, df: pd.DataFrame, lookback=1440, initial_balance=10000.0, 
                 buy_fee_rate=0.00002, sell_fee_rate=0.00003, slippage=0.0005,
                 max_holding_duration=180):  # 3시간 = 180분
        super(TradingEnv, self).__init__()
        
        self.df = df
        self.lookback = lookback  # 하루치 데이터를 관측값으로 사용
        self.max_step = len(df) - 1
        self.current_step = self.lookback
        
        self.initial_balance = initial_balance
        self.balance = self.initial_balance
        self.holding = 0.0  # 비율 기반
        self.net_worth = self.balance
        
        self.buy_fee_rate = buy_fee_rate
        self.sell_fee_rate = sell_fee_rate
        self.slippage = slippage  # 매수/매도 가격 변동 반영
        
        # 보유 기간 제한 변수
        self.max_holding_duration = max_holding_duration
        self.holding_duration = 0
        
        # 에피소드 누적 수익 추적 (실제 금액 단위)
        self.episode_profit = 0.0
        
        # (lookback, len(FEATURE_COLUMNS))
        self.observation_space = spaces.Box(
            low=-np.inf, 
            high=np.inf, 
            shape=(lookback, len(FEATURE_COLUMNS)), 
            dtype=np.float32
        )
        
        self.action_space = spaces.Box(low=0, high=1, shape=(1,), dtype=np.float32)

    def _get_current_price(self):
        return self.df.loc[self.current_step, 'close']
    
    def _get_obs(self):
        obs = self.df.loc[self.current_step - self.lookback:self.current_step-1, FEATURE_COLUMNS].values
        return obs.astype(np.float32)
    
    def _update_net_worth(self, current_price):
        self.net_worth = self.balance + (self.holding * current_price)

    def step(self, action):
        prev_net_worth = self.net_worth
        current_price = self._get_current_price()
        
        trade_amount = action[0]  # 0~1 비율
        
        # 거래 타입 초기화 (매수, 매도, 홀드)
        trade_type = "hold"
        if trade_amount > 0.5:  # 매수
            trade_type = "buy"
            amount = (trade_amount - 0.5) * 2  # 0.5~1.0 -> 0~1 변환
            cost = amount * current_price * (1 + self.buy_fee_rate + self.slippage)
            if self.balance >= cost:
                self.balance -= cost
                self.holding += amount
        elif trade_amount < 0.5:  # 매도
            trade_type = "sell"
            amount = (0.5 - trade_amount) * 2  # 0~0.5 -> 0~1 변환
            proceeds = amount * current_price * (1 - self.sell_fee_rate - self.slippage)
            if self.holding >= amount:
                self.balance += proceeds
                self.holding -= amount

        # 보유 상태에 따른 보유 시간 업데이트
        if self.holding > 0:
            self.holding_duration += 1
        else:
            self.holding_duration = 0

        # 보유 기간 초과 시 강제 매도 처리 후 에피소드 종료
        if self.holding_duration >= self.max_holding_duration:
            # 보유한 모든 포지션을 현재 가격에 강제 매도
            if self.holding > 0:
                amount = self.holding
                proceeds = amount * current_price * (1 - self.sell_fee_rate - self.slippage)
                self.balance += proceeds
                self.holding = 0.0
            done = True
        else:
            self.current_step += 1
            done = self.current_step >= self.max_step
        
        self._update_net_worth(current_price)
        
        # 실제 금액 차이를 reward로 사용 (예: -100, +1000 등)
        reward = (self.net_worth - prev_net_worth)
        
        # 에피소드 누적 수익 업데이트
        self.episode_profit += reward
        
        obs = self._get_obs()
        
        info = {
            'step': self.current_step, 
            'balance': self.balance, 
            'holding': self.holding, 
            'net_worth': self.net_worth, 
            'holding_duration': self.holding_duration,
            'trade_type': trade_type,       # "buy", "sell", 또는 "hold"
            'reward': reward                # 해당 step의 보상값 (실제 금액 차이)
        }
        # 에피소드 종료 시 누적 수익을 info에 추가
        if done:
            info['episode_profit'] = self.episode_profit
        return obs, reward, done, info

    def reset(self):
        self.current_step = self.lookback
        self.balance = self.initial_balance
        self.holding = 0.0
        self.net_worth = self.balance
        self.holding_duration = 0
        self.episode_profit = 0.0
        return self._get_obs()

###############################################
# 학습, 검증, 테스트 환경 준비
###############################################

train_env = DummyVecEnv([lambda: TradingEnv(train_df)])
val_env   = DummyVecEnv([lambda: TradingEnv(val_df)])
test_env  = DummyVecEnv([lambda: TradingEnv(test_df)])

###############################################
# 텐서보드에 커스텀 metric 기록하는 Callback
###############################################

class TensorboardCallback(BaseCallback):
    def __init__(self, verbose=0):
        super(TensorboardCallback, self).__init__(verbose)
    
    def _on_step(self) -> bool:
        infos = self.locals.get("infos", [])
        rewards = self.locals.get("rewards", [])
        dones = self.locals.get("dones", [])
        
        if infos:
            info = infos[0]
            # 현재 step의 실제 보상 (금액)
            step_reward = rewards[0] if rewards else 0.0
            self.logger.record("custom/step_reward", step_reward)
            # 현재 net_worth 기록
            net_worth = info.get("net_worth", 0)
            self.logger.record("custom/net_worth", net_worth)
            # 거래 타입 indicator 기록
            trade_type = info.get("trade_type", "hold")
            buy_flag = 1 if trade_type == "buy" else 0
            sell_flag = 1 if trade_type == "sell" else 0
            hold_flag = 1 if trade_type == "hold" else 0
            self.logger.record("custom/buy_flag", buy_flag)
            self.logger.record("custom/sell_flag", sell_flag)
            self.logger.record("custom/hold_flag", hold_flag)
            # 에피소드 종료 시 누적 수익 기록
            if dones and dones[0] and "episode_profit" in info:
                ep_profit = info["episode_profit"]
                self.logger.record("custom/episode_profit", ep_profit)
            
            # 1000 스텝마다 모델 파라미터 히스토그램 기록
            if self.n_calls % 1000 == 0:
                for name, param in self.model.policy.named_parameters():
                    for output_format in self.logger.output_formats:
                        if hasattr(output_format, "writer"):
                            output_format.writer.add_histogram(
                                f"parameters/{name}",
                                param.detach().cpu().numpy(),
                                self.num_timesteps
                            )
        return True

###############################################
# PPO 모델 설정 및 학습
###############################################

model = PPO(
    "MlpPolicy",
    train_env,
    verbose=1,
    tensorboard_log="../tensorboard_log/",
    n_steps=3200,
    gamma=0.99,
    gae_lambda=0.95,
    ent_coef=0.02,
    learning_rate=5e-5,
    clip_range=0.15
)

# 학습 시 콜백을 통해 커스텀 metric 기록
model.learn(total_timesteps=20000000, callback=TensorboardCallback())
print("----- 학습 완료 -----")

# 학습된 모델 저장
model.save("ppo_trading_model_ppo1")
print("모델 저장 완료!")

###############################################
# 검증 (Validation) & 테스트 (Test) 시뮬레이션
###############################################

for env, name in zip([val_env, test_env], ["Validation", "Test"]):
    print(f"\n===== {name} 시뮬레이션 =====")
    obs = env.reset()
    done = False
    while not done:
        action, _ = model.predict(obs)
        obs, reward, done, info = env.step(action)
    print(f"{name} 종료. 마지막 info:", info)
이 코드는

원본 CSV 파일에서 데이터를 읽고 날짜에 따라 train/validation/test 데이터로 분할합니다.
각 데이터셋에 대해 가격의 로그수익률를 계산하고, 누적합으로 조정 가격을 생성하며, 거래량은 로그 변환 후 훈련 데이터를 기준으로 표준화합니다.
최종적으로 조정된 가격(시가, 고가, 저가, 종가)과 표준화된 거래량을 FEATURE_COLUMNS에 맞게 재설정하여 강화학습 환경에 입력합니다.
나머지 강화학습 환경, 하이퍼파라미터, 학습 및 검증/테스트 과정은 기존 코드와 동일하게 사용합니다.
필요에 따라 캔들차트 스타일 지정(mpf.make_mpf_style) 부분은 시각화할 때 활용하면 됩니다.