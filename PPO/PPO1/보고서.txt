1. 소스코드 개요
(1) 데이터 전처리와 분할
원본 CSV 데이터를 읽어와 train_df, val_df, test_df로 날짜 구간별 분할.
거래량 volume에 로그 변환(np.log1p)을 적용하고, open, high, low, close, volume 피처에 대하여 StandardScaler로 표준화 진행.
(2) 강화학습 환경(TradingEnv) 정의
상태 관측값: lookback 길이(기본 1440분)만큼의 (open, high, low, close, volume) 데이터를 스택하여 전달.
조치(action): 0~1 범위를 가지는 연속값 하나를 출력 → 이를 매수/매도/유지로 해석.
0.5보다 크면 매수, 0.5보다 작으면 매도, 그리고 0.5 부근이면 유지.
매수/매도 시 슬리피지(slippage)와 수수료(buy_fee_rate, sell_fee_rate)를 반영.
보상(reward): 매 스텝에서의 net_worth 변화분(=실제 금액 차액)을 사용.
종료 조건:
데이터의 끝에 도달(current_step >= self.max_step)
최대 보유기간(max_holding_duration) 초과 시 강제 매도 후 종료
(3) PPO 알고리즘 설정
PPO(MlpPolicy):
n_steps=3200, gamma=0.99, gae_lambda=0.95, ent_coef=0.02, learning_rate=5e-5, clip_range=0.15
상술한 하이퍼파라미터로 2천만 스텝(total_timesteps=20,000,000) 학습
콜백(TensorboardCallback):
매 스텝에서 매수/매도/유지 여부, net_worth, 스텝 보상, 에피소드 종료 시 episode_profit 등을 텐서보드에 기록.
일정 스텝(1000단위)마다 파라미터 히스토그램을 기록.
2. 텐서보드 결과 분석
(1) custom/episode_profit의 저조
학습이 진행되는 동안 episode_profit이 장기적으로 상승 추세를 보이지 않고, 오히려 매우 낮아지거나 들쑥날쑥한 패턴이 나타남.
이는 다음과 같은 가능성을 시사:
환경의 보상 구조가 너무 단순해서(순간 시점의 자산 변동만 사용) 장기적인 수익 창출 방향을 찾기 어려울 수 있음.
하이퍼파라미터(학습률, 엔트로피 보너스 등) 부적절로 인해 학습 중 탐색-수렴이 제대로 이뤄지지 않았을 수 있음.
데이터 특성: 거래량이나 가격 변동이 짧은 기간에 급격하게 변하는 구간이 많아, 에이전트가 실제 이익을 내기 어렵거나, 환경 노이즈(랜덤성)가 커서 안정적인 학습이 힘들었을 수 있음.
(2) train/explained_variance의 하락
explained_variance는 Value function(Critic)이 실제 리턴(returns)을 얼마나 잘 설명하는지 나타내는 척도입니다.
학습 초기에는 어느 정도 높은 값을 유지하다가 후반에 떨어진다는 것은 Value function이 목표값(리턴)을 잘 예측 못하게 되었다는 의미입니다.
이 값이 크게 하락하면, 정책(Actor)과 가치함수(Critic) 간의 학습 불균형이 생기거나, 지나친 오버피팅 혹은 불안정한 탐색이 일어날 가능성이 있습니다.
(3) 기타 지표
train/loss, train/value_loss 등이 전반적으로 감소 추세를 보이는 것은 학습이 어느 정도 이뤄지고 있음을 시사하나, 단순히 손실이 줄었다고 해서 실제 환경에서의 성능이 보장되는 것은 아닙니다.
train/std 값이 후반부로 갈수록 매우 크게 상승 → PPO 정책의 학습 과정에서 샘플들의 행동 분포가 점차 넓어지는(=분산이 커지는) 현상을 뜻함.
이는 과도한 탐색 상태에 빠졌을 가능성도 있고, 혹은 Loss나 Value function이 불안정해지면서 정책 분포가 극단적으로 변하고 있음을 나타낼 수도 있습니다.
3. 개선 아이디어
보상 구조 수정

단순히 매 스텝의 net_worth 증감만 사용하면, 보유 기간이 긴 에이전트에게 불리할 수 있고, 반대로 잦은 거래를 선호하는 문제가 생길 수도 있습니다.
매 스텝 보상 대신 에피소드 말미의 최종 수익률 중심 보상, 혹은 거래 빈도/리스크(변동성)에 대한 패널티 등을 추가적으로 고려해 볼 수 있습니다.
학습률/기타 하이퍼파라미터 튜닝

learning_rate=5e-5도 적절할 수 있지만, 특정 시점 후 Value function 예측이 어그러진다면 더 작게 줄이거나, 학습 초기/중기 스케줄링을 고려해 볼 수 있습니다.
ent_coef=0.02도 상황에 따라 너무 높아 오히려 탐색이 과도할 수 있음. 적절히 조정(0.01 이하)하거나, clip_range(현재 0.15) 등도 조금씩 조정하면서 성능을 모니터링해 보세요.
환경 설계 측면

최대 보유 기간 max_holding_duration을 제한하는 설정이 실제로는 매우 짧아(3시간) 의사결정을 지나치게 자주 바꿔야 할 수도 있습니다.
Slippage(슬리피지), 수수료(fee) 등 실제 시장 체감을 반영하려는 것은 좋지만, 시뮬레이션에서 효과가 너무 과도하게 작용한다면 수익 창출이 어렵거나 학습이 꼬일 수 있으니, 적절히 완화 혹은 조정이 필요합니다.
특징 공학(feature engineering)

단순히 시가/고가/저가/종가/거래량을 lookback 기간만큼 쌓아둔 상태 관측은 시장의 추세나 패턴을 에이전트가 바로 인식하기 어려울 수 있음.
예) 이동평균선(MA), RSI, MACD 등 대표적 기술 지표를 추가로 계산해 넣어주면 학습 성능이 개선될 수 있습니다.
학습 모니터링 및 조기중단(Early Stopping)

학습 후반에 explained_variance가 급격히 떨어지거나 episode_profit이 계속 낮아지는 시점에서, 오히려 모델이 망가진 상태(디버그 필요)일 수 있음.
체크포인트를 주기적으로 저장한 뒤, 벨리데이션 환경에서 성능이 떨어지면 그 전 모델로 롤백하는 방식을 고려해 보세요.
4. 마무리
이 코드는 PPO를 활용한 간단한 암호화폐(또는 주가) 트레이딩 강화학습 시뮬레이션 예제입니다.
텐서보드 로그에서 episode_profit과 explained_variance가 저조하게 나타난다는 것은, 현재 설정과 보상 구조, 환경 설계가 충분히 수익을 내는 전략을 학습하기엔 부족하다는 뜻입니다.
보상 설계, 하이퍼파라미터 튜닝, 추가 지표 활용 등을 통해 성능을 개선할 수 있으며, 실제 시장 시나리오를 최대한 제대로 반영할 수 있도록 환경 세팅도 다듬어 보시는 것을 추천합니다.