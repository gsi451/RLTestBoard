<p>1. 소스코드 개요</p>

<p>(1) 데이터 전처리와 분할</p>

<ul data-end="234" data-start="57">
	<li data-end="118" data-start="57">원본 CSV 데이터를 읽어와 train_df, val_df, test_df로 날짜 구간별 분할.</li>
	<li data-end="234" data-start="119">거래량 volume에 로그 변환(np.log1p)을 적용하고, open, high, low, close, volume 피처에 대하여 StandardScaler로 표준화 진행.</li>
</ul>

<p>(2) 강화학습 환경(TradingEnv) 정의</p>

<ul data-end="708" data-start="269">
	<li data-end="356" data-start="269">상태 관측값: lookback 길이(기본 1440분)만큼의 (open, high, low, close, volume) 데이터를 스택하여 전달.</li>
	<li data-end="536" data-start="357">조치(action): 0~1 범위를 가지는 연속값 하나를 출력 &rarr; 이를 매수/매도/유지로 해석.

		<ul data-end="536" data-start="419">
			<li data-end="464" data-start="419">0.5보다 크면 매수, 0.5보다 작으면 매도, 그리고 0.5 부근이면 유지.</li>
			<li data-end="536" data-start="467">매수/매도 시 슬리피지(slippage)와 수수료(buy_fee_rate, sell_fee_rate)를 반영.</li>
		</ul>
	</li>
	<li data-end="594" data-start="537">보상(reward): 매 스텝에서의 net_worth 변화분(=실제 금액 차액)을 사용.</li>
	<li data-end="708" data-start="595">종료 조건:

		<ul data-end="708" data-start="611">
			<li data-end="656" data-start="611">데이터의 끝에 도달(current_step &gt;= self.max_step)</li>
			<li data-end="708" data-start="659">최대 보유기간(max_holding_duration) 초과 시 강제 매도 후 종료</li>
		</ul>
	</li>
</ul>

<p>(3) PPO 알고리즘 설정</p>

<ul data-end="1071" data-start="730">
	<li data-end="919" data-start="730">PPO(MlpPolicy):

		<ul data-end="919" data-start="755">
			<li data-end="862" data-start="755">n_steps=3200, gamma=0.99, gae_lambda=0.95, ent_coef=0.02, learning_rate=5e-5, clip_range=0.15</li>
			<li data-end="919" data-start="865">상술한 하이퍼파라미터로 2천만 스텝(total_timesteps=20,000,000) 학습</li>
		</ul>
	</li>
	<li data-end="1071" data-start="920">콜백(TensorboardCallback):

		<ul data-end="1071" data-start="954">
			<li data-end="1035" data-start="954">매 스텝에서 매수/매도/유지 여부, net_worth, 스텝 보상, 에피소드 종료 시 episode_profit 등을 텐서보드에 기록.</li>
			<li data-end="1071" data-start="1038">일정 스텝(1000단위)마다 파라미터 히스토그램을 기록.</li>
		</ul>
	</li>
</ul>

<p>2. 텐서보드 결과 분석</p>

<p>(1) custom/episode_profit의 저조</p>

<ul data-end="1496" data-start="1132">
	<li data-end="1212" data-start="1132">학습이 진행되는 동안 episode_profit이 장기적으로 상승 추세를 보이지 않고, 오히려 매우 낮아지거나 들쑥날쑥한 패턴이 나타남.</li>
	<li data-end="1496" data-start="1213">이는 다음과 같은 가능성을 시사:

		<ol data-end="1496" data-start="1236">
			<li data-end="1309" data-start="1236">환경의 보상 구조가 너무 단순해서(순간 시점의 자산 변동만 사용) 장기적인 수익 창출 방향을 찾기 어려울 수 있음.</li>
			<li data-end="1384" data-start="1312">하이퍼파라미터(학습률, 엔트로피 보너스 등) 부적절로 인해 학습 중 탐색-수렴이 제대로 이뤄지지 않았을 수 있음.</li>
			<li data-end="1496" data-start="1387">데이터 특성: 거래량이나 가격 변동이 짧은 기간에 급격하게 변하는 구간이 많아, 에이전트가 실제 이익을 내기 어렵거나, 환경 노이즈(랜덤성)가 커서 안정적인 학습이 힘들었을 수 있음.</li>
		</ol>
	</li>
</ul>

<p>(2) train/explained_variance의 하락</p>

<ul data-end="1816" data-start="1537">
	<li data-end="1628" data-start="1537">explained_variance는 Value function(Critic)이 실제 리턴(returns)을 얼마나 잘 설명하는지 나타내는 척도입니다.</li>
	<li data-end="1719" data-start="1629">학습 초기에는 어느 정도 높은 값을 유지하다가 후반에 떨어진다는 것은 Value function이 목표값(리턴)을 잘 예측 못하게 되었다는 의미입니다.</li>
	<li data-end="1816" data-start="1720">이 값이 크게 하락하면, 정책(Actor)과 가치함수(Critic) 간의 학습 불균형이 생기거나, 지나친 오버피팅 혹은 불안정한 탐색이 일어날 가능성이 있습니다.</li>
</ul>

<p>(3) 기타 지표</p>

<ul data-end="2156" data-start="1832">
	<li data-end="1961" data-start="1832">train/loss, train/value_loss 등이 전반적으로 감소 추세를 보이는 것은 학습이 어느 정도 이뤄지고 있음을 시사하나, 단순히 손실이 줄었다고 해서 실제 환경에서의 성능이 보장되는 것은 아닙니다.</li>
	<li data-end="2156" data-start="1962">train/std 값이 후반부로 갈수록 매우 크게 상승 &rarr; PPO 정책의 학습 과정에서 샘플들의 행동 분포가 점차 넓어지는(=분산이 커지는) 현상을 뜻함.

		<ul data-end="2156" data-start="2061">
			<li data-end="2156" data-start="2061">이는 과도한 탐색 상태에 빠졌을 가능성도 있고, 혹은 Loss나 Value function이 불안정해지면서 정책 분포가 극단적으로 변하고 있음을 나타낼 수도 있습니다.</li>
		</ul>
	</li>
</ul>

<p>3. 개선 아이디어</p>

<ol data-end="3307" data-start="2178">
	<li data-end="2380" data-start="2178">

		<p>보상 구조 수정</p>

		<ul data-end="2380" data-start="2199">
			<li data-end="2292" data-start="2199">단순히 매 스텝의 net_worth 증감만 사용하면, 보유 기간이 긴 에이전트에게 불리할 수 있고, 반대로 잦은 거래를 선호하는 문제가 생길 수도 있습니다.</li>
			<li data-end="2380" data-start="2296">매 스텝 보상 대신 에피소드 말미의 최종 수익률 중심 보상, 혹은 거래 빈도/리스크(변동성)에 대한 패널티 등을 추가적으로 고려해 볼 수 있습니다.</li>
		</ul>
	</li>
	<li data-end="2645" data-start="2382">

		<p>학습률/기타 하이퍼파라미터 튜닝</p>

		<ul data-end="2645" data-start="2412">
			<li data-end="2520" data-start="2412">learning_rate=5e-5도 적절할 수 있지만, 특정 시점 후 Value function 예측이 어그러진다면 더 작게 줄이거나, 학습 초기/중기 스케줄링을 고려해 볼 수 있습니다.</li>
			<li data-end="2645" data-start="2524">ent_coef=0.02도 상황에 따라 너무 높아 오히려 탐색이 과도할 수 있음. 적절히 조정(0.01 이하)하거나, clip_range(현재 0.15) 등도 조금씩 조정하면서 성능을 모니터링해 보세요.</li>
		</ul>
	</li>
	<li data-end="2887" data-start="2647">

		<p>환경 설계 측면</p>

		<ul data-end="2887" data-start="2668">
			<li data-end="2758" data-start="2668">최대 보유 기간 max_holding_duration을 제한하는 설정이 실제로는 매우 짧아(3시간) 의사결정을 지나치게 자주 바꿔야 할 수도 있습니다.</li>
			<li data-end="2887" data-start="2762">Slippage(슬리피지), 수수료(fee) 등 실제 시장 체감을 반영하려는 것은 좋지만, 시뮬레이션에서 효과가 너무 과도하게 작용한다면 수익 창출이 어렵거나 학습이 꼬일 수 있으니, 적절히 완화 혹은 조정이 필요합니다.</li>
		</ul>
	</li>
	<li data-end="3087" data-start="2889">

		<p>특징 공학(feature engineering)</p>

		<ul data-end="3087" data-start="2928">
			<li data-end="3013" data-start="2928">단순히 시가/고가/저가/종가/거래량을 lookback 기간만큼 쌓아둔 상태 관측은 시장의 추세나 패턴을 에이전트가 바로 인식하기 어려울 수 있음.</li>
			<li data-end="3087" data-start="3017">예) 이동평균선(MA), RSI, MACD 등 대표적 기술 지표를 추가로 계산해 넣어주면 학습 성능이 개선될 수 있습니다.</li>
		</ul>
	</li>
	<li data-end="3307" data-start="3089">

		<p>학습 모니터링 및 조기중단(Early Stopping)</p>

		<ul data-end="3307" data-start="3132">
			<li data-end="3236" data-start="3132">학습 후반에 explained_variance가 급격히 떨어지거나 episode_profit이 계속 낮아지는 시점에서, 오히려 모델이 망가진 상태(디버그 필요)일 수 있음.</li>
			<li data-end="3307" data-start="3240">체크포인트를 주기적으로 저장한 뒤, 벨리데이션 환경에서 성능이 떨어지면 그 전 모델로 롤백하는 방식을 고려해 보세요.</li>
		</ul>
	</li>
</ol>

<p>4. 마무리</p>

<ul data-end="3627" data-is-last-node="" data-is-only-node="" data-start="3325">
	<li data-end="3384" data-start="3325">이 코드는 PPO를 활용한 간단한 암호화폐(또는 주가) 트레이딩 강화학습 시뮬레이션 예제입니다.</li>
	<li data-end="3505" data-start="3385">텐서보드 로그에서 episode_profit과 explained_variance가 저조하게 나타난다는 것은, 현재 설정과 보상 구조, 환경 설계가 충분히 수익을 내는 전략을 학습하기엔 부족하다는 뜻입니다.</li>
	<li data-end="3627" data-is-last-node="" data-start="3506">보상 설계, 하이퍼파라미터 튜닝, 추가 지표 활용 등을 통해 성능을 개선할 수 있으며, 실제 시장 시나리오를 최대한 제대로 반영할 수 있도록 환경 세팅도 다듬어 보시는 것을 추천합니다.</li>
</ul>

<p><img src="https://github.com/gsi451/RLTestBoard/blob/main/PPO/PPO1/001.runs.png?raw=true" style="width: 100%; object-fit: cover;" class="fr-fic fr-dii"></p>

<p><img src="https://github.com/gsi451/RLTestBoard/blob/main/PPO/PPO1/002.custom.png?raw=true" style="width: 100%; object-fit: cover;" class="fr-fic fr-dii"></p>

<p><img src="https://github.com/gsi451/RLTestBoard/blob/main/PPO/PPO1/003.train.png?raw=true" style="width: 100%; object-fit: cover;" class="fr-fic fr-dii"></p>

<p><img src="https://github.com/gsi451/RLTestBoard/blob/main/PPO/PPO1/004.train.png?raw=true" style="width: 100%; object-fit: cover;" class="fr-fic fr-dii"></p>

<p>
	<br>
</p>
